{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please use the following code to visualize the results trained by recipeClassifierHATT_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text\n",
    "import numpy as np\n",
    "gloveW2V = '../../dir_HugeFiles/glove.6B/glove.6B.100d.txt'\n",
    "vocab = '../data/vocab.txt'\n",
    "gloveW2V = vocab\n",
    "f = open(gloveW2V)\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.layers import Layer\n",
    "from keras.models import load_model\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "\n",
    "#model = load_model('../../dir_HugeFiles/snap_0307/attention/1e-4model_e001_v--0.229_model.h5',\n",
    "#                  custom_objects ={'AttLayer':AttLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load essential modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from nltk import tokenize\n",
    "#### additional\n",
    "import pickle\n",
    "from args import get_parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "from sys import exit\n",
    "\n",
    "def save_pickle(filename, obj, overwrite = False):\n",
    "    make_dir(filename)\n",
    "    if os.path.isfile(filename) == True and overwrite == False:\n",
    "        print('already exists'+filename)\n",
    "    else:\n",
    "        with open(filename, 'wb') as gfp:\n",
    "            pickle.dump(obj, gfp, protocol=2)\n",
    "            gfp.close()\n",
    "            \n",
    "def make_dir(filename):\n",
    "    dir_path = os.path.dirname(filename)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print('make dir')\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as gfp:\n",
    "        r = pickle.load(gfp)\n",
    "    return r\n",
    "\n",
    "def test(X_test, y_test, model, threshold, print_ = True):\n",
    "    prob_class1 = model.predict(X_test)[:,1]\n",
    "    return validate(prob_class1, y_test, threshold, print_ = True)\n",
    "\n",
    "def validate(prob_class1, y_test, threshold, print_ = True):\n",
    "    preds = [1 if i>threshold else 0 for i in prob_class1]\n",
    "    true = y_test.argmax(axis = -1).tolist()\n",
    "    f1 = f1_score(true, preds)\n",
    "    if print_ == True:\n",
    "        print(':::current prob threshold %.3f '%(threshold))\n",
    "        print('   positive number: pred %d, true %d' %(sum(preds), sum(true)))\n",
    "        print('   -f1 %.3f, precision %.3f, recall %.3f' % (f1, precision_score(true, preds), recall_score(true, preds)))\n",
    "    return -f1 # return negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load saved opts, so that saved model will be loaded together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = '../../dir_HugeFiles/snap_0307/attention/1e-4model_e001_v--0.229'\n",
    "statename =resume + '.pickle'\n",
    "state = load_pickle(statename)\n",
    "opts = state['opts']\n",
    "opts.resume = resume\n",
    "opts.gpu= 1\n",
    "\n",
    "#modelname = opts.resume+'_model.h5'\n",
    "#if os.path.isfile(modelname):\n",
    "#    print(\"=> loading checkpoint '{}'\".format(opts.resume))\n",
    "#    loaded_model.load_weights(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the model, then load the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current running on GPU number:', 1)\n",
      "tag of tensor board: snap_0307/attention/1e-4\n",
      "Total 10211 unique tokens.\n",
      "('Shape of data tensor:', (55102, 15, 100))\n",
      "('Shape of label tensor:', (55102, 2))\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[32498.000 562.000]\n",
      "[10833.000 188.000]\n",
      "class weight is 57.8\n",
      "Total 400000 word vectors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100) into shape (300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b0588aaf1737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m embedding_layer = Embedding(len(word_index) + 1,\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100) into shape (300)"
     ]
    }
   ],
   "source": [
    "gpu_id = opts.gpu\n",
    "print('Current running on GPU number:', gpu_id)\n",
    "gpu_options = tf.GPUOptions(visible_device_list=str(gpu_id))\n",
    "config = tf.ConfigProto(device_count = {'GPU': gpu_id, 'CPU': 10},\n",
    "                        gpu_options = gpu_options,\n",
    "                        intra_op_parallelism_threads = 32,\n",
    "                        inter_op_parallelism_threads = 32)\n",
    "sess = tf.Session(config = config)\n",
    "K.set_session(sess)\n",
    "####\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "##### customized settings ##### \n",
    "p = os.path.abspath(opts.snapshots)\n",
    "tag = '/'.join(p.split(os.sep)[3:])# store the path, but w/o prefix workspace/dir_HugeFiles\n",
    "print('tag of tensor board: %s'%(tag))\n",
    "\n",
    "reviews, labels = load_pickle(opts.train)\n",
    "\n",
    "# if want to use less data to train\n",
    "small = opts.small\n",
    "if small:\n",
    "    reviews, labels = reviews[:5000], labels[:5000]\n",
    "texts = [' '.join(recipe) for recipe in reviews]\n",
    "'''\n",
    "# sentences = list of string, each string contains one sentence\n",
    "# texts =  flatten sentences, separate by recipes\n",
    "# reviews = list of sentences\n",
    "reviews = [v['directions'] for v in dic.values()]\n",
    "texts = [' '.join(v['directions']) for v in dic.values()]\n",
    "labels = [v['GI'] for v in dic.values()]    \n",
    "'''\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify = labels, test_size = 0.2, random_state = opts.random)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25, random_state = 1 + opts.random)\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)\n",
    "\n",
    "# delete variable to release memory\n",
    "del data\n",
    "\n",
    "class_wights = opts.pweight\n",
    "# if -1, then automatically caculate the balanced weight\n",
    "if class_wights == -1:\n",
    "    class_01 = y_train.sum(axis= 0)\n",
    "    class_weights = round(class_01[0]/class_01[1],1)\n",
    "print('class weight is %.1f' % class_weights)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(opts.gloveW2V)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))  \n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "modelname = opts.resume+'_model.h5'\n",
    "if os.path.isfile(modelname):\n",
    "    print(\"=> loading checkpoint '{}'\".format(opts.resume))\n",
    "    model.load_weights(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a new layer that will export the weights on words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer_visual(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer_visual, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer_visual, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return ait\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "document_id= 0\n",
    "l_att_sent_vis = AttLayer_visual(100)(l_lstm_sent)\n",
    "model_vis = Model(review_input, l_att_sent_vis)\n",
    "ait_sent = model_vis.predict(X_test)\n",
    "\n",
    "# only takes one document\n",
    "l_att_vis = AttLayer_visual(100)(l_lstm)\n",
    "sentEncoder_vis = Model(sentence_input, l_att_vis)\n",
    "ait = sentEncoder_vis.predict(X_test[document_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the word-index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentences importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.071: noodles romanoff ii\n",
      "Score 0.075: 1 8 ounce package wide egg noodles\n",
      "Score 0.079: 2 cups sour cream\n",
      "Score 0.079: 1 4 cup grated parmesan cheese\n",
      "Score 0.062: 1 tablespoon chopped fresh chives\n",
      "Score 0.056: 1 teaspoon salt\n",
      "Score 0.054: 1 8 teaspoon ground black pepper\n",
      "Score 0.055: 1 clove crushed garlic\n",
      "Score 0.070: 2 tablespoons butter\n",
      "Score 0.066: 1 4 cup grated parmesan cheese\n",
      "Score 0.071: in a large pot with boiling salted water cook egg noodles until al dente\n",
      "Score 0.066: drain\n",
      "Score 0.059: in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic\n",
      "Score 0.068: stir in butter or margarine to hot egg noodles\n",
      "Score 0.070: stir in sour cream mixture\n"
     ]
    }
   ],
   "source": [
    "def display_sent(document_id, ait_sent, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            print('Score %.3f: %s' %(score, str_sent))\n",
    "display_sent(document_id, ait_sent, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentence and word level importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n"
     ]
    }
   ],
   "source": [
    "def plt_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    rows2 = []\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            row = [score]+[reverse_word_map[t] for t in sentence]\n",
    "            pads = max_sen - len(row)\n",
    "            row+=pads*[]\n",
    "            rows.append(row)\n",
    "            rows2.append([score] + [' '.join([reverse_word_map[t] for t in sentence])])\n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            score_2 = (np.sqrt(score)*np.array(weight)).tolist()\n",
    "            row = [0]+score_2\n",
    "            pads = max_sen - len(row)\n",
    "            row+=pads*[]\n",
    "            #rows.append(row)\n",
    "    df = pd.DataFrame(rows2)\n",
    "    return df\n",
    "df = plt_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.071:       noodles romanoff ii\n",
      "pw:               [0.381, 0.312, 0.306]\n",
      "sqrt ps *pw       [0.101 0.083 0.081]\n",
      "Score 0.075:       1 8 ounce package wide egg noodles\n",
      "pw:               [0.148, 0.142, 0.145, 0.138, 0.128, 0.126, 0.173]\n",
      "sqrt ps *pw       [0.041 0.039 0.040 0.038 0.035 0.035 0.048]\n",
      "Score 0.079:       2 cups sour cream\n",
      "pw:               [0.301, 0.258, 0.216, 0.224]\n",
      "sqrt ps *pw       [0.084 0.072 0.061 0.063]\n",
      "Score 0.079:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.201, 0.195, 0.178, 0.139, 0.143, 0.144]\n",
      "sqrt ps *pw       [0.057 0.055 0.050 0.039 0.040 0.041]\n",
      "Score 0.062:       1 tablespoon chopped fresh chives\n",
      "pw:               [0.239, 0.209, 0.187, 0.189, 0.176]\n",
      "sqrt ps *pw       [0.059 0.052 0.046 0.047 0.044]\n",
      "Score 0.056:       1 teaspoon salt\n",
      "pw:               [0.372, 0.304, 0.324]\n",
      "sqrt ps *pw       [0.088 0.072 0.077]\n",
      "Score 0.054:       1 8 teaspoon ground black pepper\n",
      "pw:               [0.176, 0.176, 0.153, 0.145, 0.158, 0.191]\n",
      "sqrt ps *pw       [0.041 0.041 0.035 0.034 0.037 0.044]\n",
      "Score 0.055:       1 clove crushed garlic\n",
      "pw:               [0.282, 0.234, 0.24, 0.244]\n",
      "sqrt ps *pw       [0.066 0.055 0.056 0.057]\n",
      "Score 0.070:       2 tablespoons butter\n",
      "pw:               [0.372, 0.315, 0.313]\n",
      "sqrt ps *pw       [0.099 0.083 0.083]\n",
      "Score 0.066:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.201, 0.195, 0.178, 0.139, 0.143, 0.144]\n",
      "sqrt ps *pw       [0.052 0.050 0.046 0.036 0.037 0.037]\n",
      "Score 0.071:       in a large pot with boiling salted water cook egg noodles until al dente\n",
      "pw:               [0.074, 0.077, 0.07, 0.074, 0.076, 0.073, 0.077, 0.076, 0.071, 0.062, 0.081, 0.071, 0.06, 0.057]\n",
      "sqrt ps *pw       [0.020 0.021 0.019 0.020 0.020 0.019 0.021 0.020 0.019 0.017 0.022 0.019\n",
      " 0.016 0.015]\n",
      "Score 0.066:       drain\n",
      "pw:               [1.0]\n",
      "sqrt ps *pw       [0.256]\n",
      "Score 0.059:       in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic\n",
      "pw:               [0.046, 0.048, 0.046, 0.043, 0.046, 0.044, 0.044, 0.037, 0.035, 0.044, 0.043, 0.038, 0.039, 0.041, 0.034, 0.03, 0.031, 0.036, 0.043, 0.037, 0.038, 0.048, 0.054, 0.053]\n",
      "sqrt ps *pw       [0.011 0.012 0.011 0.010 0.011 0.011 0.011 0.009 0.008 0.011 0.010 0.009\n",
      " 0.009 0.010 0.008 0.007 0.008 0.009 0.010 0.009 0.009 0.012 0.013 0.013]\n",
      "Score 0.068:       stir in butter or margarine to hot egg noodles\n",
      "pw:               [0.102, 0.11, 0.1, 0.108, 0.102, 0.122, 0.122, 0.101, 0.133]\n",
      "sqrt ps *pw       [0.027 0.029 0.026 0.028 0.027 0.032 0.032 0.026 0.035]\n",
      "Score 0.070:       stir in sour cream mixture\n",
      "pw:               [0.196, 0.231, 0.193, 0.18, 0.2]\n",
      "sqrt ps *pw       [0.052 0.061 0.051 0.048 0.053]\n"
     ]
    }
   ],
   "source": [
    "def display_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            score_2 = np.sqrt(score)*np.array(weight)\n",
    "            print('Score %.3f:       %s' %(score, str_sent))\n",
    "            print('pw:               %s' %(weight))\n",
    "            print('sqrt ps *pw       %s' %(score_2))\n",
    "display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.071:       noodles romanoff ii\n",
      "pw:               [0.381, 0.312, 0.306]\n",
      "sqrt ps *pw       [0.101 0.083 0.081]\n",
      "Score 0.075:       1 8 ounce package wide egg noodles\n",
      "pw:               [0.148, 0.142, 0.145, 0.138, 0.128, 0.126, 0.173]\n",
      "sqrt ps *pw       [0.041 0.039 0.040 0.038 0.035 0.035 0.048]\n",
      "Score 0.079:       2 cups sour cream\n",
      "pw:               [0.301, 0.258, 0.216, 0.224]\n",
      "sqrt ps *pw       [0.084 0.072 0.061 0.063]\n",
      "Score 0.079:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.201, 0.195, 0.178, 0.139, 0.143, 0.144]\n",
      "sqrt ps *pw       [0.057 0.055 0.050 0.039 0.040 0.041]\n",
      "Score 0.062:       1 tablespoon chopped fresh chives\n",
      "pw:               [0.239, 0.209, 0.187, 0.189, 0.176]\n",
      "sqrt ps *pw       [0.059 0.052 0.046 0.047 0.044]\n",
      "Score 0.056:       1 teaspoon salt\n",
      "pw:               [0.372, 0.304, 0.324]\n",
      "sqrt ps *pw       [0.088 0.072 0.077]\n",
      "Score 0.054:       1 8 teaspoon ground black pepper\n",
      "pw:               [0.176, 0.176, 0.153, 0.145, 0.158, 0.191]\n",
      "sqrt ps *pw       [0.041 0.041 0.035 0.034 0.037 0.044]\n",
      "Score 0.055:       1 clove crushed garlic\n",
      "pw:               [0.282, 0.234, 0.24, 0.244]\n",
      "sqrt ps *pw       [0.066 0.055 0.056 0.057]\n",
      "Score 0.070:       2 tablespoons butter\n",
      "pw:               [0.372, 0.315, 0.313]\n",
      "sqrt ps *pw       [0.099 0.083 0.083]\n",
      "Score 0.066:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.201, 0.195, 0.178, 0.139, 0.143, 0.144]\n",
      "sqrt ps *pw       [0.052 0.050 0.046 0.036 0.037 0.037]\n",
      "Score 0.071:       in a large pot with boiling salted water cook egg noodles until al dente\n",
      "pw:               [0.074, 0.077, 0.07, 0.074, 0.076, 0.073, 0.077, 0.076, 0.071, 0.062, 0.081, 0.071, 0.06, 0.057]\n",
      "sqrt ps *pw       [0.020 0.021 0.019 0.020 0.020 0.019 0.021 0.020 0.019 0.017 0.022 0.019\n",
      " 0.016 0.015]\n",
      "Score 0.066:       drain\n",
      "pw:               [1.0]\n",
      "sqrt ps *pw       [0.256]\n",
      "Score 0.059:       in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic\n",
      "pw:               [0.046, 0.048, 0.046, 0.043, 0.046, 0.044, 0.044, 0.037, 0.035, 0.044, 0.043, 0.038, 0.039, 0.041, 0.034, 0.03, 0.031, 0.036, 0.043, 0.037, 0.038, 0.048, 0.054, 0.053]\n",
      "sqrt ps *pw       [0.011 0.012 0.011 0.010 0.011 0.011 0.011 0.009 0.008 0.011 0.010 0.009\n",
      " 0.009 0.010 0.008 0.007 0.008 0.009 0.010 0.009 0.009 0.012 0.013 0.013]\n",
      "Score 0.068:       stir in butter or margarine to hot egg noodles\n",
      "pw:               [0.102, 0.11, 0.1, 0.108, 0.102, 0.122, 0.122, 0.101, 0.133]\n",
      "sqrt ps *pw       [0.027 0.029 0.026 0.028 0.027 0.032 0.032 0.026 0.035]\n",
      "Score 0.070:       stir in sour cream mixture\n",
      "pw:               [0.196, 0.231, 0.193, 0.18, 0.2]\n",
      "sqrt ps *pw       [0.052 0.061 0.051 0.048 0.053]\n"
     ]
    }
   ],
   "source": [
    "def display_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            score_2 = np.sqrt(score)*np.array(weight)\n",
    "            print('Score %.3f:       %s' %(score, str_sent))\n",
    "            print('pw:               %s' %(weight))\n",
    "            print('sqrt ps *pw       %s' %(score_2))\n",
    "display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: the recipe that has the highest prob to be True (Low GI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6999\n"
     ]
    }
   ],
   "source": [
    "document_id = np.argmax(prob[:,1])\n",
    "print(document_id)\n",
    "ait = sentEncoder_vis.predict(X_test[document_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.071:       noodles romanoff ii\n",
      "pw:               [0.381, 0.312, 0.306]\n",
      "sqrt ps *pw       [0.101 0.083 0.081]\n",
      "Score 0.075:       1 8 ounce package wide egg noodles\n",
      "pw:               [0.148, 0.142, 0.145, 0.138, 0.128, 0.126, 0.173]\n",
      "sqrt ps *pw       [0.041 0.039 0.040 0.038 0.035 0.035 0.048]\n",
      "Score 0.079:       2 cups sour cream\n",
      "pw:               [0.301, 0.258, 0.216, 0.224]\n",
      "sqrt ps *pw       [0.084 0.072 0.061 0.063]\n",
      "Score 0.079:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.201, 0.195, 0.178, 0.139, 0.143, 0.144]\n",
      "sqrt ps *pw       [0.057 0.055 0.050 0.039 0.040 0.041]\n",
      "Score 0.062:       1 tablespoon chopped fresh chives\n",
      "pw:               [0.239, 0.209, 0.187, 0.189, 0.176]\n",
      "sqrt ps *pw       [0.059 0.052 0.046 0.047 0.044]\n",
      "Score 0.056:       1 teaspoon salt\n",
      "pw:               [0.372, 0.304, 0.324]\n",
      "sqrt ps *pw       [0.088 0.072 0.077]\n",
      "Score 0.054:       1 8 teaspoon ground black pepper\n",
      "pw:               [0.176, 0.176, 0.153, 0.145, 0.158, 0.191]\n",
      "sqrt ps *pw       [0.041 0.041 0.035 0.034 0.037 0.044]\n",
      "Score 0.055:       1 clove crushed garlic\n",
      "pw:               [0.282, 0.234, 0.24, 0.244]\n",
      "sqrt ps *pw       [0.066 0.055 0.056 0.057]\n",
      "Score 0.070:       2 tablespoons butter\n",
      "pw:               [0.372, 0.315, 0.313]\n",
      "sqrt ps *pw       [0.099 0.083 0.083]\n",
      "Score 0.066:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.201, 0.195, 0.178, 0.139, 0.143, 0.144]\n",
      "sqrt ps *pw       [0.052 0.050 0.046 0.036 0.037 0.037]\n",
      "Score 0.071:       in a large pot with boiling salted water cook egg noodles until al dente\n",
      "pw:               [0.074, 0.077, 0.07, 0.074, 0.076, 0.073, 0.077, 0.076, 0.071, 0.062, 0.081, 0.071, 0.06, 0.057]\n",
      "sqrt ps *pw       [0.020 0.021 0.019 0.020 0.020 0.019 0.021 0.020 0.019 0.017 0.022 0.019\n",
      " 0.016 0.015]\n",
      "Score 0.066:       drain\n",
      "pw:               [1.0]\n",
      "sqrt ps *pw       [0.256]\n",
      "Score 0.059:       in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic\n",
      "pw:               [0.046, 0.048, 0.046, 0.043, 0.046, 0.044, 0.044, 0.037, 0.035, 0.044, 0.043, 0.038, 0.039, 0.041, 0.034, 0.03, 0.031, 0.036, 0.043, 0.037, 0.038, 0.048, 0.054, 0.053]\n",
      "sqrt ps *pw       [0.011 0.012 0.011 0.010 0.011 0.011 0.011 0.009 0.008 0.011 0.010 0.009\n",
      " 0.009 0.010 0.008 0.007 0.008 0.009 0.010 0.009 0.009 0.012 0.013 0.013]\n",
      "Score 0.068:       stir in butter or margarine to hot egg noodles\n",
      "pw:               [0.102, 0.11, 0.1, 0.108, 0.102, 0.122, 0.122, 0.101, 0.133]\n",
      "sqrt ps *pw       [0.027 0.029 0.026 0.028 0.027 0.032 0.032 0.026 0.035]\n",
      "Score 0.070:       stir in sour cream mixture\n",
      "pw:               [0.196, 0.231, 0.193, 0.18, 0.2]\n",
      "sqrt ps *pw       [0.052 0.061 0.051 0.048 0.053]\n"
     ]
    }
   ],
   "source": [
    "df = display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_74c29d20_4701_11e9_bc67_67590879b57crow0_col0 {\n",
       "            background-color:  #df2179;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow1_col0 {\n",
       "            background-color:  #a10346;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow2_col0 {\n",
       "            background-color:  #6f0025;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow3_col0 {\n",
       "            background-color:  #67001f;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow4_col0 {\n",
       "            background-color:  #cfa8d2;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow5_col0 {\n",
       "            background-color:  #eae5f1;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow6_col0 {\n",
       "            background-color:  #f7f4f9;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow7_col0 {\n",
       "            background-color:  #f1edf5;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow8_col0 {\n",
       "            background-color:  #e32581;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow9_col0 {\n",
       "            background-color:  #db6db4;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow10_col0 {\n",
       "            background-color:  #db1e71;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow11_col0 {\n",
       "            background-color:  #d873b7;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow12_col0 {\n",
       "            background-color:  #dcc9e2;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow13_col0 {\n",
       "            background-color:  #e34a9f;\n",
       "        }    #T_74c29d20_4701_11e9_bc67_67590879b57crow14_col0 {\n",
       "            background-color:  #e72989;\n",
       "        }</style>  \n",
       "<table id=\"T_74c29d20_4701_11e9_bc67_67590879b57c\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >sentence importance</th> \n",
       "        <th class=\"col_heading level0 col1\" >recipe</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow0_col0\" class=\"data row0 col0\" >0.0706836</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow0_col1\" class=\"data row0 col1\" >noodles romanoff ii</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow1_col0\" class=\"data row1 col0\" >0.0754744</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow1_col1\" class=\"data row1 col1\" >1 8 ounce package wide egg noodles</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow2_col0\" class=\"data row2 col0\" >0.0787379</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow2_col1\" class=\"data row2 col1\" >2 cups sour cream</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow3_col0\" class=\"data row3 col0\" >0.0792366</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow3_col1\" class=\"data row3 col1\" >1 4 cup grated parmesan cheese</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow4_col0\" class=\"data row4 col0\" >0.0615562</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow4_col1\" class=\"data row4 col1\" >1 tablespoon chopped fresh chives</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow5_col0\" class=\"data row5 col0\" >0.0562454</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow5_col1\" class=\"data row5 col1\" >1 teaspoon salt</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow6_col0\" class=\"data row6 col0\" >0.0537145</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow6_col1\" class=\"data row6 col1\" >1 8 teaspoon ground black pepper</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row7\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow7_col0\" class=\"data row7 col0\" >0.0549511</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow7_col1\" class=\"data row7 col1\" >1 clove crushed garlic</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row8\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow8_col0\" class=\"data row8 col0\" >0.0702124</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow8_col1\" class=\"data row8 col1\" >2 tablespoons butter</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row9\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow9_col0\" class=\"data row9 col0\" >0.0659451</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow9_col1\" class=\"data row9 col1\" >1 4 cup grated parmesan cheese</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row10\" class=\"row_heading level0 row10\" >10</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow10_col0\" class=\"data row10 col0\" >0.0712006</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow10_col1\" class=\"data row10 col1\" >in a large pot with boiling salted water cook egg noodles until al dente</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row11\" class=\"row_heading level0 row11\" >11</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow11_col0\" class=\"data row11 col0\" >0.0655702</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow11_col1\" class=\"data row11 col1\" >drain</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row12\" class=\"row_heading level0 row12\" >12</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow12_col0\" class=\"data row12 col0\" >0.0588547</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow12_col1\" class=\"data row12 col1\" >in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row13\" class=\"row_heading level0 row13\" >13</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow13_col0\" class=\"data row13 col0\" >0.0679454</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow13_col1\" class=\"data row13 col1\" >stir in butter or margarine to hot egg noodles</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_74c29d20_4701_11e9_bc67_67590879b57clevel0_row14\" class=\"row_heading level0 row14\" >14</th> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow14_col0\" class=\"data row14 col0\" >0.0696719</td> \n",
       "        <td id=\"T_74c29d20_4701_11e9_bc67_67590879b57crow14_col1\" class=\"data row14 col1\" >stir in sour cream mixture</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f10c1bdf810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['sentence importance', 'recipe']\n",
    "df.style.background_gradient(cmap = 'PuRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
