{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please use the following code to visualize the results trained by recipeClassifierHATT_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load essential modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from nltk import tokenize\n",
    "#### additional\n",
    "import pickle\n",
    "from args import get_parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "from sys import exit\n",
    "\n",
    "def save_pickle(filename, obj, overwrite = False):\n",
    "    make_dir(filename)\n",
    "    if os.path.isfile(filename) == True and overwrite == False:\n",
    "        print('already exists'+filename)\n",
    "    else:\n",
    "        with open(filename, 'wb') as gfp:\n",
    "            pickle.dump(obj, gfp, protocol=2)\n",
    "            gfp.close()\n",
    "            \n",
    "def make_dir(filename):\n",
    "    dir_path = os.path.dirname(filename)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print('make dir')\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as gfp:\n",
    "        r = pickle.load(gfp)\n",
    "    return r\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def test(X_test, y_test, model, threshold, print_ = True):\n",
    "    prob_class1 = model.predict(X_test)[:,1]\n",
    "    return validate(prob_class1, y_test, threshold, print_ = True)\n",
    "\n",
    "def validate(prob_class1, y_test, threshold, print_ = True):\n",
    "    preds = [1 if i>threshold else 0 for i in prob_class1]\n",
    "    true = y_test.argmax(axis = -1).tolist()\n",
    "    f1 = f1_score(true, preds)\n",
    "    if print_ == True:\n",
    "        print(':::current prob threshold %.3f '%(threshold))\n",
    "        print('   positive number: pred %d, true %d' %(sum(preds), sum(true)))\n",
    "        print('   -f1 %.3f, precision %.3f, recall %.3f' % (f1, precision_score(true, preds), recall_score(true, preds)))\n",
    "    return -f1 # return negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load saved opts, so that saved model will be loaded together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = '../../dir_HugeFiles/snap_0311/attention/model_e006_v--0.370'\n",
    "statename =resume + '.pickle'\n",
    "state = load_pickle(statename)\n",
    "opts = state['opts']\n",
    "opts.resume = resume\n",
    "opts.gpu= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the model, then load the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current running on GPU number:', 1)\n",
      "tag of tensor board: snap_0311/attention\n"
     ]
    }
   ],
   "source": [
    "gpu_id = opts.gpu\n",
    "print('Current running on GPU number:', gpu_id)\n",
    "gpu_options = tf.GPUOptions(visible_device_list=str(gpu_id))\n",
    "config = tf.ConfigProto(device_count = {'GPU': gpu_id, 'CPU': 10},\n",
    "                        gpu_options = gpu_options,\n",
    "                        intra_op_parallelism_threads = 32,\n",
    "                        inter_op_parallelism_threads = 32)\n",
    "sess = tf.Session(config = config)\n",
    "K.set_session(sess)\n",
    "####\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "##### customized settings ##### \n",
    "p = os.path.abspath(opts.snapshots)\n",
    "tag = '/'.join(p.split(os.sep)[3:])# store the path, but w/o prefix workspace/dir_HugeFiles\n",
    "print('tag of tensor board: %s'%(tag))\n",
    "\n",
    "reviews, labels = load_pickle(opts.train)\n",
    "\n",
    "# if want to use less data to train\n",
    "small = opts.small\n",
    "if small:\n",
    "    reviews, labels = reviews[:5000], labels[:5000]\n",
    "texts = [' '.join(recipe) for recipe in reviews]\n",
    "'''\n",
    "# sentences = list of string, each string contains one sentence\n",
    "# texts =  flatten sentences, separate by recipes\n",
    "# reviews = list of sentences\n",
    "reviews = [v['directions'] for v in dic.values()]\n",
    "texts = [' '.join(v['directions']) for v in dic.values()]\n",
    "labels = [v['GI'] for v in dic.values()]    \n",
    "'''\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify = labels, test_size = 0.2, random_state = opts.random)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25, random_state = 1 + opts.random)\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)\n",
    "\n",
    "# delete variable to release memory\n",
    "del data\n",
    "\n",
    "class_wights = opts.pweight\n",
    "# if -1, then automatically caculate the balanced weight\n",
    "if class_wights == -1:\n",
    "    class_01 = y_train.sum(axis= 0)\n",
    "    class_weights = round(class_01[0]/class_01[1],1)\n",
    "print('class weight is %.1f' % class_weights)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(opts.gloveW2V)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))  \n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "modelname = opts.resume+'_model.h5'\n",
    "if os.path.isfile(modelname):\n",
    "    print(\"=> loading checkpoint '{}'\".format(opts.resume))\n",
    "    model.load_weights(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a new layer that will export the weights on words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer_visual(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer_visual, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer_visual, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return ait\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "document_id= 0\n",
    "l_att_sent_vis = AttLayer_visual(100)(l_lstm_sent)\n",
    "model_vis = Model(review_input, l_att_sent_vis)\n",
    "ait_sent = model_vis.predict(X_test)\n",
    "\n",
    "# only takes one document\n",
    "l_att_vis = AttLayer_visual(100)(l_lstm)\n",
    "sentEncoder_vis = Model(sentence_input, l_att_vis)\n",
    "ait = sentEncoder_vis.predict(X_test[document_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the word-index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentences importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sent(document_id, ait_sent, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            print('Score %.3f: %s' %(score, str_sent))\n",
    "display_sent(document_id, ait_sent, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentence and word level importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            score_2 = np.sqrt(score)*np.array(weight)\n",
    "            print('Score %.3f:       %s' %(score, str_sent))\n",
    "            print('pw:               %s' %(weight))\n",
    "            print('sqrt ps *pw       %s' %(score_2))\n",
    "display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            row = [score]+[reverse_word_map[t] for t in sentence]\n",
    "            pads = max_sen - len(row)\n",
    "            row+=pads*[]\n",
    "            rows.append(row)\n",
    "            \n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            score_2 = (np.sqrt(score)*np.array(weight)).tolist()\n",
    "            row = [0]+score_2\n",
    "            pads = max_sen - len(row)\n",
    "            row+=pads*[]\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "df = plt_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = np.argmax(prob[:,1])\n",
    "print(document_id)\n",
    "ait = sentEncoder_vis.predict(X_test[document_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
