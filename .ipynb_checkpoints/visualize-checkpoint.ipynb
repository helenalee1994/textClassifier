{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please use the following code to visualize the results trained by recipeClassifierHATT_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load essential modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from nltk import tokenize\n",
    "#### additional\n",
    "import pickle\n",
    "from args import get_parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "from sys import exit\n",
    "\n",
    "def save_pickle(filename, obj, overwrite = False):\n",
    "    make_dir(filename)\n",
    "    if os.path.isfile(filename) == True and overwrite == False:\n",
    "        print('already exists'+filename)\n",
    "    else:\n",
    "        with open(filename, 'wb') as gfp:\n",
    "            pickle.dump(obj, gfp, protocol=2)\n",
    "            gfp.close()\n",
    "            \n",
    "def make_dir(filename):\n",
    "    dir_path = os.path.dirname(filename)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print('make dir')\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as gfp:\n",
    "        r = pickle.load(gfp)\n",
    "    return r\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def test(X_test, y_test, model, threshold, print_ = True):\n",
    "    prob_class1 = model.predict(X_test)[:,1]\n",
    "    return validate(prob_class1, y_test, threshold, print_ = True)\n",
    "\n",
    "def validate(prob_class1, y_test, threshold, print_ = True):\n",
    "    preds = [1 if i>threshold else 0 for i in prob_class1]\n",
    "    true = y_test.argmax(axis = -1).tolist()\n",
    "    f1 = f1_score(true, preds)\n",
    "    if print_ == True:\n",
    "        print(':::current prob threshold %.3f '%(threshold))\n",
    "        print('   positive number: pred %d, true %d' %(sum(preds), sum(true)))\n",
    "        print('   -f1 %.3f, precision %.3f, recall %.3f' % (f1, precision_score(true, preds), recall_score(true, preds)))\n",
    "    return -f1 # return negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load saved opts, so that saved model will be loaded together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = '../../dir_HugeFiles/snap_0311/attention/model_e006_v--0.370'\n",
    "statename =resume + '.pickle'\n",
    "state = load_pickle(statename)\n",
    "opts = state['opts']\n",
    "opts.resume = resume\n",
    "opts.gpu= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the model, then load the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current running on GPU number:', 1)\n",
      "tag of tensor board: snap_0311/attention\n",
      "Total 18234 unique tokens.\n",
      "('Shape of data tensor:', (55102, 15, 100))\n",
      "('Shape of label tensor:', (55102, 2))\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[32498.000 562.000]\n",
      "[10833.000 188.000]\n",
      "class weight is 57.8\n",
      "Total 400000 word vectors.\n",
      "=> loading checkpoint '../../dir_HugeFiles/snap_0311/attention/model_e006_v--0.370'\n"
     ]
    }
   ],
   "source": [
    "gpu_id = opts.gpu\n",
    "print('Current running on GPU number:', gpu_id)\n",
    "gpu_options = tf.GPUOptions(visible_device_list=str(gpu_id))\n",
    "config = tf.ConfigProto(device_count = {'GPU': gpu_id, 'CPU': 10},\n",
    "                        gpu_options = gpu_options,\n",
    "                        intra_op_parallelism_threads = 32,\n",
    "                        inter_op_parallelism_threads = 32)\n",
    "sess = tf.Session(config = config)\n",
    "K.set_session(sess)\n",
    "####\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "##### customized settings ##### \n",
    "p = os.path.abspath(opts.snapshots)\n",
    "tag = '/'.join(p.split(os.sep)[3:])# store the path, but w/o prefix workspace/dir_HugeFiles\n",
    "print('tag of tensor board: %s'%(tag))\n",
    "\n",
    "reviews, labels = load_pickle(opts.train)\n",
    "\n",
    "# if want to use less data to train\n",
    "small = opts.small\n",
    "if small:\n",
    "    reviews, labels = reviews[:5000], labels[:5000]\n",
    "texts = [' '.join(recipe) for recipe in reviews]\n",
    "'''\n",
    "# sentences = list of string, each string contains one sentence\n",
    "# texts =  flatten sentences, separate by recipes\n",
    "# reviews = list of sentences\n",
    "reviews = [v['directions'] for v in dic.values()]\n",
    "texts = [' '.join(v['directions']) for v in dic.values()]\n",
    "labels = [v['GI'] for v in dic.values()]    \n",
    "'''\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify = labels, test_size = 0.2, random_state = opts.random)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25, random_state = 1 + opts.random)\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)\n",
    "\n",
    "# delete variable to release memory\n",
    "del data\n",
    "\n",
    "class_wights = opts.pweight\n",
    "# if -1, then automatically caculate the balanced weight\n",
    "if class_wights == -1:\n",
    "    class_01 = y_train.sum(axis= 0)\n",
    "    class_weights = round(class_01[0]/class_01[1],1)\n",
    "print('class weight is %.1f' % class_weights)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(opts.gloveW2V)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))  \n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "modelname = opts.resume+'_model.h5'\n",
    "if os.path.isfile(modelname):\n",
    "    print(\"=> loading checkpoint '{}'\".format(opts.resume))\n",
    "    model.load_weights(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a new layer that will export the weights on words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer_visual(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer_visual, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer_visual, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return ait\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "document_id= 0\n",
    "l_att_sent_vis = AttLayer_visual(100)(l_lstm_sent)\n",
    "model_vis = Model(review_input, l_att_sent_vis)\n",
    "ait_sent = model_vis.predict(X_test)\n",
    "\n",
    "# only takes one document\n",
    "l_att_vis = AttLayer_visual(100)(l_lstm)\n",
    "sentEncoder_vis = Model(sentence_input, l_att_vis)\n",
    "ait = sentEncoder_vis.predict(X_test[document_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the word-index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentences importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.061: noodles romanoff ii\n",
      "Score 0.065: 1 8 ounce package wide egg noodles\n",
      "Score 0.067: 2 cups sour cream\n",
      "Score 0.065: 1 4 cup grated parmesan cheese\n",
      "Score 0.054: 1 tablespoon chopped fresh chives\n",
      "Score 0.052: 1 teaspoon salt\n",
      "Score 0.055: 1 8 teaspoon ground black pepper\n",
      "Score 0.052: 1 clove crushed garlic\n",
      "Score 0.056: 2 tablespoons butter\n",
      "Score 0.080: 1 4 cup grated parmesan cheese\n",
      "Score 0.096: in a large pot with boiling salted water cook egg noodles until al dente\n",
      "Score 0.071: drain\n",
      "Score 0.073: in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic\n",
      "Score 0.080: stir in butter or margarine to hot egg noodles\n",
      "Score 0.075: stir in sour cream mixture\n"
     ]
    }
   ],
   "source": [
    "def display_sent(document_id, ait_sent, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            print('Score %.3f: %s' %(score, str_sent))\n",
    "display_sent(document_id, ait_sent, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentence and word level importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.061:       noodles romanoff ii\n",
      "pw:               [0.325, 0.362, 0.314]\n",
      "sqrt ps *pw       [0.080 0.089 0.077]\n",
      "Score 0.065:       1 8 ounce package wide egg noodles\n",
      "pw:               [0.119, 0.118, 0.152, 0.153, 0.136, 0.187, 0.135]\n",
      "sqrt ps *pw       [0.030 0.030 0.039 0.039 0.035 0.048 0.034]\n",
      "Score 0.067:       2 cups sour cream\n",
      "pw:               [0.2, 0.175, 0.236, 0.388]\n",
      "sqrt ps *pw       [0.052 0.045 0.061 0.100]\n",
      "Score 0.065:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.144, 0.133, 0.106, 0.152, 0.222, 0.244]\n",
      "sqrt ps *pw       [0.037 0.034 0.027 0.039 0.056 0.062]\n",
      "Score 0.054:       1 tablespoon chopped fresh chives\n",
      "pw:               [0.193, 0.194, 0.178, 0.18, 0.255]\n",
      "sqrt ps *pw       [0.045 0.045 0.041 0.042 0.059]\n",
      "Score 0.052:       1 teaspoon salt\n",
      "pw:               [0.353, 0.341, 0.306]\n",
      "sqrt ps *pw       [0.080 0.078 0.070]\n",
      "Score 0.055:       1 8 teaspoon ground black pepper\n",
      "pw:               [0.166, 0.158, 0.162, 0.164, 0.19, 0.16]\n",
      "sqrt ps *pw       [0.039 0.037 0.038 0.038 0.044 0.037]\n",
      "Score 0.052:       1 clove crushed garlic\n",
      "pw:               [0.269, 0.28, 0.224, 0.228]\n",
      "sqrt ps *pw       [0.061 0.064 0.051 0.052]\n",
      "Score 0.056:       2 tablespoons butter\n",
      "pw:               [0.272, 0.295, 0.433]\n",
      "sqrt ps *pw       [0.064 0.070 0.103]\n",
      "Score 0.080:       1 4 cup grated parmesan cheese\n",
      "pw:               [0.144, 0.133, 0.106, 0.152, 0.222, 0.244]\n",
      "sqrt ps *pw       [0.041 0.038 0.030 0.043 0.063 0.069]\n",
      "Score 0.096:       in a large pot with boiling salted water cook egg noodles until al dente\n",
      "pw:               [0.054, 0.056, 0.061, 0.068, 0.058, 0.074, 0.065, 0.065, 0.065, 0.106, 0.083, 0.075, 0.077, 0.093]\n",
      "sqrt ps *pw       [0.017 0.017 0.019 0.021 0.018 0.023 0.020 0.020 0.020 0.033 0.026 0.023\n",
      " 0.024 0.029]\n",
      "Score 0.071:       drain\n",
      "pw:               [1.0]\n",
      "sqrt ps *pw       [0.266]\n",
      "Score 0.073:       in a large bowl mix together the sour cream 1 4 cup of the grated parmesan cheese chives salt ground black pepper and garlic\n",
      "pw:               [0.034, 0.034, 0.035, 0.032, 0.034, 0.036, 0.037, 0.044, 0.066, 0.042, 0.037, 0.031, 0.033, 0.034, 0.038, 0.06, 0.066, 0.053, 0.043, 0.046, 0.049, 0.041, 0.035, 0.039]\n",
      "sqrt ps *pw       [0.009 0.009 0.009 0.009 0.009 0.010 0.010 0.012 0.018 0.011 0.010 0.008\n",
      " 0.009 0.009 0.010 0.016 0.018 0.014 0.012 0.012 0.013 0.011 0.009 0.011]\n",
      "Score 0.080:       stir in butter or margarine to hot egg noodles\n",
      "pw:               [0.091, 0.086, 0.116, 0.115, 0.126, 0.109, 0.118, 0.136, 0.103]\n",
      "sqrt ps *pw       [0.026 0.024 0.033 0.033 0.036 0.031 0.033 0.039 0.029]\n",
      "Score 0.075:       stir in sour cream mixture\n",
      "pw:               [0.166, 0.156, 0.157, 0.282, 0.239]\n",
      "sqrt ps *pw       [0.045 0.043 0.043 0.077 0.065]\n"
     ]
    }
   ],
   "source": [
    "def display_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            score_2 = np.sqrt(score)*np.array(weight)\n",
    "            print('Score %.3f:       %s' %(score, str_sent))\n",
    "            print('pw:               %s' %(weight))\n",
    "            print('sqrt ps *pw       %s' %(score_2))\n",
    "display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n"
     ]
    }
   ],
   "source": [
    "def plt_importance(document_id, ait_sent, ait, X, y):\n",
    "    print('Label as %r Low GI recipe' % (y[document_id][1] == 1))\n",
    "    max_sen = 15\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    rows2 = []\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            row = [score]+[reverse_word_map[t] for t in sentence]\n",
    "            pads = max_sen - len(row)\n",
    "            row+=pads*[]\n",
    "            rows.append(row)\n",
    "            rows2.append([score] + [' '.join([reverse_word_map[t] for t in sentence])])\n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            score_2 = (np.sqrt(score)*np.array(weight)).tolist()\n",
    "            row = [0]+score_2\n",
    "            pads = max_sen - len(row)\n",
    "            row+=pads*[]\n",
    "            #rows.append(row)\n",
    "    df = pd.DataFrame(rows2)\n",
    "    return df\n",
    "df = plt_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow0_col0 {\n",
       "            background-color:  #67001f;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow1_col0 {\n",
       "            background-color:  #de67b1;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow2_col0 {\n",
       "            background-color:  #e3489e;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow3_col0 {\n",
       "            background-color:  #e3d9eb;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow4_col0 {\n",
       "            background-color:  #ede8f3;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow5_col0 {\n",
       "            background-color:  #dfcfe6;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow6_col0 {\n",
       "            background-color:  #eae5f1;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow7_col0 {\n",
       "            background-color:  #f7f4f9;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow8_col0 {\n",
       "            background-color:  #cfaad2;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow9_col0 {\n",
       "            background-color:  #f6f3f8;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow10_col0 {\n",
       "            background-color:  #d183bf;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow11_col0 {\n",
       "            background-color:  #c993c7;\n",
       "        }    #T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow12_col0 {\n",
       "            background-color:  #d71a68;\n",
       "        }</style>  \n",
       "<table id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19f\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >sentence importance</th> \n",
       "        <th class=\"col_heading level0 col1\" >recipe</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow0_col0\" class=\"data row0 col0\" >0.0792336</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow0_col1\" class=\"data row0 col1\" >lentils with tomatoes</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow1_col0\" class=\"data row1 col0\" >0.0702889</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow1_col1\" class=\"data row1 col1\" >1 quart water</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow2_col0\" class=\"data row2 col0\" >0.0714434</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow2_col1\" class=\"data row2 col1\" >1 cup dry lentils</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow3_col0\" class=\"data row3 col0\" >0.0642198</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow3_col1\" class=\"data row3 col1\" >3 tablespoons olive oil</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow4_col0\" class=\"data row4 col0\" >0.0630272</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow4_col1\" class=\"data row4 col1\" >1 medium green bell pepper chopped</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow5_col0\" class=\"data row5 col0\" >0.0647682</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow5_col1\" class=\"data row5 col1\" >1 medium onion chopped</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow6_col0\" class=\"data row6 col0\" >0.0633201</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow6_col1\" class=\"data row6 col1\" >2 1 2 cups peeled seeded and chopped tomatoes</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row7\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow7_col0\" class=\"data row7 col0\" >0.0615798</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow7_col1\" class=\"data row7 col1\" >salt and pepper to taste</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row8\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow8_col0\" class=\"data row8 col0\" >0.0669419</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow8_col1\" class=\"data row8 col1\" >in a pot bring the water to a boil and stir in the lentils</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row9\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow9_col0\" class=\"data row9 col0\" >0.0617426</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow9_col1\" class=\"data row9 col1\" >reduce heat and simmer 20 minutes drain</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row10\" class=\"row_heading level0 row10\" >10</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow10_col0\" class=\"data row10 col0\" >0.0690222</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow10_col1\" class=\"data row10 col1\" >heat the olive oil in a large skillet over medium heat and saute the green bell pepper and onion until tender</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row11\" class=\"row_heading level0 row11\" >11</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow11_col0\" class=\"data row11 col0\" >0.0682225</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow11_col1\" class=\"data row11 col1\" >mix in the tomatoes and season with salt and pepper</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19flevel0_row12\" class=\"row_heading level0 row12\" >12</th> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow12_col0\" class=\"data row12 col0\" >0.0740599</td> \n",
       "        <td id=\"T_83719f0e_4406_11e9_84e4_cbdc50a6d19frow12_col1\" class=\"data row12 col1\" >stir in the lentils reduce heat and simmer 25 to 30 minutes until the lentils are tender</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faf96e08cd0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['sentence importance', 'recipe']\n",
    "df.style.background_gradient(cmap = 'PuRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6999\n"
     ]
    }
   ],
   "source": [
    "document_id = np.argmax(prob[:,1])\n",
    "print(document_id)\n",
    "ait = sentEncoder_vis.predict(X_test[document_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label as False Low GI recipe\n",
      "Score 0.079:       lentils with tomatoes\n",
      "pw:               [0.382, 0.297, 0.321]\n",
      "sqrt ps *pw       [0.108 0.084 0.090]\n",
      "Score 0.070:       1 quart water\n",
      "pw:               [0.31, 0.397, 0.293]\n",
      "sqrt ps *pw       [0.082 0.105 0.078]\n",
      "Score 0.071:       1 cup dry lentils\n",
      "pw:               [0.269, 0.21, 0.259, 0.262]\n",
      "sqrt ps *pw       [0.072 0.056 0.069 0.070]\n",
      "Score 0.064:       3 tablespoons olive oil\n",
      "pw:               [0.238, 0.245, 0.256, 0.26]\n",
      "sqrt ps *pw       [0.060 0.062 0.065 0.066]\n",
      "Score 0.063:       1 medium green bell pepper chopped\n",
      "pw:               [0.159, 0.152, 0.174, 0.193, 0.163, 0.16]\n",
      "sqrt ps *pw       [0.040 0.038 0.044 0.048 0.041 0.040]\n",
      "Score 0.065:       1 medium onion chopped\n",
      "pw:               [0.247, 0.225, 0.271, 0.256]\n",
      "sqrt ps *pw       [0.063 0.057 0.069 0.065]\n",
      "Score 0.063:       2 1 2 cups peeled seeded and chopped tomatoes\n",
      "pw:               [0.119, 0.113, 0.107, 0.098, 0.127, 0.102, 0.102, 0.118, 0.114]\n",
      "sqrt ps *pw       [0.030 0.028 0.027 0.025 0.032 0.026 0.026 0.030 0.029]\n",
      "Score 0.062:       salt and pepper to taste\n",
      "pw:               [0.2, 0.181, 0.206, 0.201, 0.211]\n",
      "sqrt ps *pw       [0.050 0.045 0.051 0.050 0.052]\n",
      "Score 0.067:       in a pot bring the water to a boil and stir in the lentils\n",
      "pw:               [0.064, 0.064, 0.073, 0.067, 0.064, 0.073, 0.067, 0.065, 0.071, 0.063, 0.087, 0.081, 0.078, 0.083]\n",
      "sqrt ps *pw       [0.017 0.017 0.019 0.017 0.017 0.019 0.017 0.017 0.018 0.016 0.023 0.021\n",
      " 0.020 0.021]\n",
      "Score 0.062:       reduce heat and simmer 20 minutes drain\n",
      "pw:               [0.161, 0.124, 0.12, 0.146, 0.146, 0.143, 0.16]\n",
      "sqrt ps *pw       [0.040 0.031 0.030 0.036 0.036 0.036 0.040]\n",
      "Score 0.069:       heat the olive oil in a large skillet over medium heat and saute the green bell pepper and onion until tender\n",
      "pw:               [0.035, 0.035, 0.047, 0.048, 0.047, 0.044, 0.047, 0.059, 0.042, 0.041, 0.044, 0.04, 0.054, 0.049, 0.058, 0.063, 0.052, 0.043, 0.049, 0.045, 0.055]\n",
      "sqrt ps *pw       [0.009 0.009 0.012 0.013 0.012 0.012 0.012 0.016 0.011 0.011 0.012 0.011\n",
      " 0.014 0.013 0.015 0.017 0.014 0.011 0.013 0.012 0.014]\n",
      "Score 0.068:       mix in the tomatoes and season with salt and pepper\n",
      "pw:               [0.11, 0.102, 0.099, 0.101, 0.093, 0.092, 0.093, 0.11, 0.092, 0.107]\n",
      "sqrt ps *pw       [0.029 0.027 0.026 0.026 0.024 0.024 0.024 0.029 0.024 0.028]\n",
      "Score 0.074:       stir in the lentils reduce heat and simmer 25 to 30 minutes until the lentils are tender\n",
      "pw:               [0.056, 0.049, 0.048, 0.051, 0.062, 0.047, 0.047, 0.057, 0.06, 0.056, 0.06, 0.058, 0.063, 0.064, 0.072, 0.074, 0.076]\n",
      "sqrt ps *pw       [0.015 0.013 0.013 0.014 0.017 0.013 0.013 0.016 0.016 0.015 0.016 0.016\n",
      " 0.017 0.017 0.020 0.020 0.021]\n"
     ]
    }
   ],
   "source": [
    "display_importance(document_id, ait_sent, ait, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
