{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from nltk import tokenize\n",
    "#### additional\n",
    "import pickle\n",
    "from args import get_parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "from sys import exit\n",
    "writer = SummaryWriter()\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "def save_pickle(filename, obj, overwrite = False):\n",
    "    make_dir(filename)\n",
    "    if os.path.isfile(filename) == True and overwrite == False:\n",
    "        print('already exists'+filename)\n",
    "    else:\n",
    "        with open(filename, 'wb') as gfp:\n",
    "            pickle.dump(obj, gfp, protocol=2)\n",
    "            gfp.close()\n",
    "            \n",
    "def make_dir(filename):\n",
    "    dir_path = os.path.dirname(filename)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print('make dir')\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as gfp:\n",
    "        r = pickle.load(gfp)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load saved opts, so that saved model will be loaded together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = '../../dir_HugeFiles/snap_0311/attention/full/model_e005_v--0.419'\n",
    "statename =resume + '.pickle'\n",
    "state = load_pickle(statename)\n",
    "opts = state['opts']\n",
    "opts.resume = resume\n",
    "opts.gpu= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.wordlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current running on GPU number:', 3)\n",
      "tag of tensor board: snap_0311/attention/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "gpu_id = opts.gpu\n",
    "print('Current running on GPU number:', gpu_id)\n",
    "gpu_options = tf.GPUOptions(visible_device_list=str(gpu_id))\n",
    "config = tf.ConfigProto(device_count = {'GPU': gpu_id, 'CPU': 10},\n",
    "                        gpu_options = gpu_options,\n",
    "                        intra_op_parallelism_threads = 32,\n",
    "                        inter_op_parallelism_threads = 32)\n",
    "sess = tf.Session(config = config)\n",
    "K.set_session(sess)\n",
    "####\n",
    "MAX_SENT_LENGTH = opts.sentlength # 100\n",
    "MAX_SENTS = opts.wordlength #15\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "##### customized settings ##### \n",
    "p = os.path.abspath(opts.snapshots)\n",
    "tag = '/'.join(p.split(os.sep)[3:])# store the path, but w/o prefix workspace/dir_HugeFiles\n",
    "print('tag of tensor board: %s'%(tag))\n",
    "\n",
    "reviews, labels = load_pickle(opts.train)\n",
    "\n",
    "# if want to use less data to train\n",
    "small = opts.small\n",
    "if small:\n",
    "    reviews, labels = reviews[:5000], labels[:5000]\n",
    "texts = [' '.join(recipe) for recipe in reviews]\n",
    "'''\n",
    "# sentences = list of string, each string contains one sentence\n",
    "# texts =  flatten sentences, separate by recipes\n",
    "# reviews = list of sentences\n",
    "reviews = [v['directions'] for v in dic.values()]\n",
    "texts = [' '.join(v['directions']) for v in dic.values()]\n",
    "labels = [v['GI'] for v in dic.values()]    \n",
    "'''\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify = labels, test_size = 0.2, random_state = opts.random)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.25, random_state = 1 + opts.random)\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)\n",
    "\n",
    "# delete variable to release memory\n",
    "del data\n",
    "\n",
    "class_wights = opts.pweight\n",
    "# if -1, then automatically caculate the balanced weight\n",
    "if class_wights == -1:\n",
    "    class_01 = y_train.sum(axis= 0)\n",
    "    class_weights = round(class_01[0]/class_01[1],1)\n",
    "print('class weight is %.1f' % class_weights)\n",
    "\n",
    "if opts.foodW2V:\n",
    "    pretrained = Word2Vec.load(opts.foodW2V)# '../data/foodvec300.model'\n",
    "    embeddings_index = pretrained.wv\n",
    "    EMBEDDING_DIM = pretrained.vector_size\n",
    "    opts.emdding_dim = EMBEDDING_DIM\n",
    "    print(opts.foodW2V)\n",
    "\n",
    "'''\n",
    "embeddings_index = {}\n",
    "f = open(opts.gloveW2V)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(embeddings_index))  \n",
    "'''\n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in pretrained.wv.vocab:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "    else:\n",
    "        # words not found in embedding index will be UNK\n",
    "        embedding_vector = embeddings_index.get_vector('UNK')\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "#SGD\n",
    "custom_optimizer = optimizers.SGD(lr=opts.lr, momentum= 0.9)\n",
    "if opts.optm:\n",
    "    custom_optimizer = opts.optm\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer = custom_optimizer, #'rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "if opts.resume:\n",
    "    modelname = opts.resume+'_model.h5'\n",
    "    statename = opts.resume+'.pickle'\n",
    "    if os.path.isfile(modelname):\n",
    "        print(\"=> loading checkpoint '{}'\".format(opts.resume))\n",
    "        model.load_weights(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer_visual(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer_visual, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer_visual, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return ait\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the word-index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visaulize sentences importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_att_sent_vis = AttLayer_visual(100)(l_lstm_sent)\n",
    "model_vis = Model(review_input, l_att_sent_vis)\n",
    "l_att_vis = AttLayer_visual(100)(l_lstm)\n",
    "sentEncoder_vis = Model(sentence_input, l_att_vis)\n",
    "\n",
    "def display_sent(document_id, X, y, max_sen = 15, color = True, words = True):\n",
    "    write_to_list = []\n",
    "    write_to_list.append('Label as %r Low GI recipe  <br />' % (y[document_id][1] == 1))\n",
    "    # only takes one document\n",
    "    d1, d2 = X[document_id].shape\n",
    "    X0 = X[document_id].reshape(1, d1, d2)\n",
    "    ait_sent = model_vis.predict(X0)\n",
    "    ait = sentEncoder_vis.predict(X[document_id])\n",
    "    \n",
    "    sents, scores, scores_2 = [], [], []\n",
    "    for i in range(max_sen):\n",
    "        sentence = [t for t in X[document_id][i] if t!=0]\n",
    "        if sentence:\n",
    "            str_sent = ' '.join([reverse_word_map[t] for t in sentence])\n",
    "            sents.append(str_sent)\n",
    "            score = ait_sent[document_id][i][0]\n",
    "            scores.append(score)\n",
    "            \n",
    "            weight = [round(ait[i][t][0],3) for t, v in enumerate(sentence)]\n",
    "            score_2 = (np.sqrt(score)*np.array(weight)).tolist() # follow the paper: use score_2\n",
    "            score_3 = [weight*len(sentence) for weight in sentence]\n",
    "            scores_2.append(score_3) # did not follow the paper: use weight\n",
    "            \n",
    "    scores_norm = [(score-min(scores))/(max(scores)-min(scores)) for score in scores]\n",
    "    maxi, mini = max(sum(scores_2, [])), min(sum(scores_2, []))\n",
    "    for i, score in enumerate(scores):\n",
    "        if not color:\n",
    "            write_to_list.append('Score %.3f: %s' %(score, sents[i]))\n",
    "        else:\n",
    "            write_to_list.append('<font style=\"background: rgba(255, 255, 0, %f)\">%.3f   </font>' % (scores_norm[i], score))\n",
    "            if not words:\n",
    "                write_to_list.append('%s  <br />'% (sents[i]))\n",
    "            else:\n",
    "                sentence = [t for t in X[document_id][i] if t!=0]\n",
    "                for j, word in enumerate(sentence):\n",
    "                    color = (scores_2[i][j]-mini)/(maxi-mini)\n",
    "                    write_to_list.append('<font style=\"background: rgba(255, 0, 255, %f)\">%s </font>' % (10*color, reverse_word_map[word]))\n",
    "                write_to_list.append('<br />')\n",
    "    return write_to_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = display_sent(0, X_test, y_test)\n",
    "with open('../figs/mypage.html', \"a+\") as html_file:\n",
    "    for sent in w:\n",
    "        html_file.write(sent)\n",
    "    html_file.write(\"<br /><br /><br />\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
